{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56195a7-6122-44a8-8bcf-040049ef5dc6",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393588ef-b81d-44c7-8e6a-255e898eb632",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import subprocess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc0989-2e27-4237-80cb-7fb0b9ae5b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ca88cf-6a8e-4e0b-ac79-53ded8d21dee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_hydro_KD = {\n",
    "                'G': -0.4,\n",
    "                'A': 1.8,\n",
    "                'V': 4.2,\n",
    "                'F': 2.8,\n",
    "                'P': -1.6,\n",
    "                'M': 1.9,\n",
    "                'I': 4.5,\n",
    "                'L': 3.8,\n",
    "                'D': -3.5,\n",
    "                'E': -3.5,\n",
    "                'K': -3.9,\n",
    "                'R': -4.5,\n",
    "                'S': -0.8,\n",
    "                'T': -0.7,\n",
    "                'Y': -1.3,\n",
    "                'H': -3.2,\n",
    "                'C': 2.5,\n",
    "                'N': -3.5,\n",
    "                'Q': -3.5,\n",
    "                'W': -0.9\n",
    "}\n",
    "\n",
    "dic_hydro_E = {\n",
    "                'G': 0.48,\n",
    "                'A': 0.62,\n",
    "                'V': 1.08,\n",
    "                'F': 1.19,\n",
    "                'P': 0.12,\n",
    "                'M': 0.64,\n",
    "                'I': 1.38,\n",
    "                'L': 1.06,\n",
    "                'D': -0.9,\n",
    "                'E': -0.74,\n",
    "                'K': -1.5,\n",
    "                'R': -2.53,\n",
    "                'S': -0.18,\n",
    "                'T': -0.05,\n",
    "                'Y': 0.26,\n",
    "                'H': -0.4,\n",
    "                'C': 0.29,\n",
    "                'N': -0.78,\n",
    "                'Q': -0.85,\n",
    "                'W': 0.81\n",
    "}\n",
    "\n",
    "dic_charge = {\n",
    "                'G': 0,\n",
    "                'A': 0,\n",
    "                'V': 0,\n",
    "                'F': 0,\n",
    "                'P': 0,\n",
    "                'M': 0,\n",
    "                'I': 0,\n",
    "                'L': 0,\n",
    "                'D': -1,\n",
    "                'E': -1,\n",
    "                'K': 1,\n",
    "                'R': 1,\n",
    "                'S': 0,\n",
    "                'T': 0,\n",
    "                'Y': 0,\n",
    "                'H': 0,\n",
    "                'C': 0,\n",
    "                'N': 0,\n",
    "                'Q': 0,\n",
    "                'W': 0, \n",
    "}\n",
    "\n",
    "dic_groupe_lens = {'A':375, 'B':375, 'C':375, 'DP':374, 'DQ':378, 'DR':374}\n",
    "dic_locus_lens = {'A':276, 'B':276, 'C':276, 'B2':99, 'DQA':186, 'DQB':192, 'DRA':182, 'DRB':192,\n",
    "                   'DPA':183, 'DPB':191}\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "pdb2fasta = f'{base_dir}/pdb2fasta.sh' \n",
    "surf_meth = 'RSASA'\n",
    "hydro_scale = 'KD'\n",
    "\n",
    "dir_data='/home/damaya/capsid_new/HLA-EpiCheck'\n",
    "\n",
    "radius_patch = 15\n",
    "thresh_surf = 20\n",
    "\n",
    "list_avail_alleles = []\n",
    "with open(f'{dir_data}/list_antigens.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        list_avail_alleles.append(line.strip().split('/')[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f38f0-6d89-426a-85e8-84c3f3a7ecb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Table #0 (PatchDescription)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bf8b6-c5d7-4196-90d4-fef30edaa8a2",
   "metadata": {},
   "source": [
    "This section computes the tables table_0* and track_resids_patchs_table_0* . The table table_0* contains the charges and hydrophobicity of the central AA of each patch. \n",
    "The table track_resids_patchs_table_0* contains the antigen and central AA of each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e48e5dd-c6e1-4e0b-99c7-ccb5db3a56ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "table_0 = pd.DataFrame(columns=['patch_ID', 'charge_+', 'charge_-', 'hydro'])\n",
    "patch_count = 0\n",
    "track_resids_patchs = pd.DataFrame(columns=['patch_ID', 'antigen', 'central_AA'])\n",
    "\n",
    "for antigen in list_avail_alleles:\n",
    "    groupe = antigen.split('_')[0] if antigen.split('_')[0] in ['A', 'B', 'C'] else \\\n",
    "                antigen.split('_')[0][:2]\n",
    "    dir_antigen = f'{dir_data}/{antigen}'\n",
    "    os.system(f'{pdb2fasta} {dir_antigen}/PDBs/*-frame_0.pdb > {dir_antigen}/PDBs/seq_frame_0.txt')\n",
    "    \n",
    "    with open(f'{dir_antigen}/PDBs/seq_frame_0.txt') as file1:\n",
    "        seq_fasta = []\n",
    "        for line in file1:\n",
    "            seq_fasta.append(line.strip())\n",
    "            \n",
    "    seq_antigen = seq_fasta[1] + seq_fasta[3]\n",
    "    if len(seq_antigen) != dic_groupe_lens[groupe]:\n",
    "        os.system(f'echo \"Error at {antigen}, len seq_antigen = {len(seq_antigen)} and dic lens = {dic_groupe_lens[groupe]}\" >> {base_dir}/log_error_lens')\n",
    "    df_surf = pd.read_csv(f'{dir_antigen}/{surf_meth}_median.txt', \\\n",
    "                                       header=0, sep='\\t', usecols=[1])\n",
    "    df_surf['index'] = range(1, dic_groupe_lens[groupe]+1)\n",
    "    df_surf.set_index('index', inplace=True)\n",
    "    col_name = 'RSASA' if surf_meth == 'RSASA' else 'mean'\n",
    "    surface_AA = df_surf.loc[df_surf[col_name] >= thresh_surf]\n",
    "    \n",
    "    for central_AA in surface_AA.index:\n",
    "        charge = dic_charge[seq_antigen[central_AA-1]]\n",
    "        charge_pos = charge if charge > 0 else 0\n",
    "        charge_neg = charge if charge < 0 else 0\n",
    "        hydro = dic_hydro_KD[seq_antigen[central_AA-1]]\n",
    "        table_0.at[patch_count] = [patch_count, charge_pos, charge_neg, hydro]\n",
    "        track_resids_patchs.at[patch_count] = [patch_count, antigen, central_AA]\n",
    "        patch_count += 1\n",
    "table_0.to_csv(f'{dir_data}/table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv')\n",
    "track_resids_patchs.to_csv(f'{dir_data}/track_resids_patchs_table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e65d9-ba73-4ab7-9dc4-489c374e67df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Table #1 (PatchFramesValues) (RSASA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477795dc-e15b-4d0e-9685-869171e8a507",
   "metadata": {},
   "source": [
    "This section computes the table table_1*. The table table_1* contains solvent-accessibility data of each patch in each frame considered. It means that patches are processed in a frame-wise way. \n",
    "For each patch, a set of features is calculated for the central AA of the patch and for the AAs belonging to the patch that are solvent-accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07781a4a-7edb-4ed3-b804-99044d02f3ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrack_frames_patchs = pd.DataFrame(columns=['frame_ID', 'patch_ID', 'frame_number'])\\ntrack_frames_patchs[['frame_ID', 'patch_ID', 'frame_number']] = list(zip(range(len(table_1['frame_ID'])),                                                                 table_1['patch_ID'],                                                                 table_1['frame_ID']))\\n\\nfor col in track_frames_patchs.columns:\\n    track_frames_patchs[col] = list(map(int, track_frames_patchs[col]))\\n\\ntrack_frames_patchs.to_csv(f'{base_dir}/prediction_epitopes/track_frames_patchs_table_1_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compute_table_1(index, col):\n",
    "    max_sasa = pd.read_csv(f'{base_dir}/../data/Max_SASA_per_residue.tsv', header=None, sep='\\t')\n",
    "    frame_count = 0\n",
    "    table_1 = pd.DataFrame(dtype='object')\n",
    "    antigen = col['antigen']\n",
    "    groupe = antigen.split('_')[0] if antigen.split('_')[0] in ['A', 'B', 'C'] else \\\n",
    "                antigen.split('_')[0][:2]\n",
    "    dir_antigen = f'{dir_data}/{antigen}'\n",
    "    \n",
    "    with open(f'{dir_antigen}/PDBs/seq_frame_0.txt') as file1:\n",
    "        seq_fasta = []\n",
    "        for line in file1:\n",
    "            seq_fasta.append(line.strip())\n",
    "    \n",
    "    seq_antigen = seq_fasta[1] + seq_fasta[3]\n",
    "    central_AA = col['central_AA']\n",
    "    patch_ID = col['patch_ID']\n",
    "    res_name_central = seq_antigen[central_AA-1]\n",
    "    list_patch_all = set()\n",
    "    dic_frame_patch = {}\n",
    "    \n",
    "    with open(f'{dir_antigen}/patchs/patches_vmd_resid_{central_AA}_size_{radius_patch}.txt', 'r') \\\n",
    "            as file1:\n",
    "        for line in file1:\n",
    "            line = line.strip()\n",
    "            list_patch_all = list_patch_all.union({int(i.strip()) for i in \\\n",
    "                                                   line.split(':')[0].split(' ')})\n",
    "            frame_number = int(line.split(':')[1].split('_')[-1].split('.pdb')[0])\n",
    "            dic_frame_patch[frame_number] = [int(i.strip()) for i in line.split(':')[0].split(' ')]\n",
    "    \n",
    "    list_patch_all.remove(central_AA)\n",
    "    \n",
    "    list_SASA_files = subprocess.check_output(f'ls {dir_antigen}/SASAs_out/SASA*txt', \\\n",
    "                                              shell=True)\n",
    "    num_frames = len(list_SASA_files.decode('utf-8').split('\\n')) - 1\n",
    "    for frame_number in sorted(dic_frame_patch.keys()):\n",
    "        if frame_number < num_frames:\n",
    "            col_table_1 = pd.Series(name=frame_count, dtype='object')\n",
    "            col_table_1['patch_ID'] = patch_ID\n",
    "            col_table_1['frame_ID'] = frame_count\n",
    "            SASA_file = pd.read_csv(f'{dir_antigen}/SASAs_out/SASA_{frame_number}.txt',\n",
    "                                usecols=[0, 1],\n",
    "                                names=[0,1],\n",
    "                                header=None,\n",
    "                                sep='\\t',\n",
    "                                low_memory=False,\n",
    "                                dtype={\n",
    "                                    0: 'int32',\n",
    "                                    1: 'float64'\n",
    "                                })\n",
    "\n",
    "            sasa = SASA_file.loc[SASA_file[0] == central_AA-1, 1].iloc[0]\n",
    "            col_table_1['RSASA_central_AA'] = (sasa / max_sasa.loc[ \\\n",
    "                                            max_sasa[0] == res_name_central].iloc[0, 3]) * 100\n",
    "            \n",
    "            for resid in list_patch_all:\n",
    "                res_name = seq_antigen[resid-1]\n",
    "                sasa = SASA_file.loc[SASA_file[0] == resid-1, 1].iloc[0]\n",
    "                rsasa = (sasa / max_sasa.loc[max_sasa[0] == res_name].iloc[0, 3]) * 100\n",
    "                col_table_1[f'RSASA_resid_{resid}'] = rsasa\n",
    "                col_table_1[f'presence_resid_{resid}'] = 1 if resid in \\\n",
    "                                                        dic_frame_patch[frame_number] else 0\n",
    "            \n",
    "            table_1 = pd.concat([table_1, col_table_1], axis=1)\n",
    "            frame_count += 1\n",
    "    return table_1.T\n",
    "\n",
    "jobs = mp.cpu_count()-2\n",
    "list_param = []\n",
    "for index, col in track_resids_patchs.iterrows():\n",
    "    list_param.append((index, col))\n",
    "\n",
    "outs_table_1 = Parallel(n_jobs=jobs)(delayed(compute_table_1)(index, col) for index, col in \\\n",
    "                                     list_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0fdc27-5353-42d9-9e60-9c8822de88fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "lst_chunks = (x for x in chunks(outs_table_1, 5000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da3de9e-7ee8-45b6-8050-95279027cf0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for df in lst_chunks:\n",
    "    var = pd.concat(df, axis=0, ignore_index=True)\n",
    "    f_table_1 = pd.HDFStore(f\"{dir_data}/table_1_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}_{count}.hdf\")\n",
    "    f_table_1.append(f\"table_1_{count}\", var)\n",
    "    f_table_1.close()\n",
    "    count += 1\n",
    "nb_files = count+1\n",
    "del lst_chunks, var, outs_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f20b95f-6995-4fb8-bc23-58a1879a7257",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_df = (pd.read_hdf(f'{dir_data}/table_1_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}_{i}.hdf', f'table_1_{i}') for i in range(nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa738d9-4da3-4e3c-88fc-af35049216aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Table #2 (PatchAggValuesPerResidue)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c359b99-f3d8-4c45-be92-d5af812528c1",
   "metadata": {},
   "source": [
    "The tables table_0* and table_1* are concatenated. Aggregation is performed with respect to the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e26a5-d11d-4e56-b381-694fd98a5d32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_N_RMSF_all = {}\n",
    "data = []\n",
    "\n",
    "for antigen in list_avail_alleles:\n",
    "    groupe = antigen.split('_')[0] if antigen.split('_')[0] in ['A', 'B', 'C'] else \\\n",
    "                antigen.split('_')[0][:2]\n",
    "    \n",
    "    if groupe in ['A', 'B', 'C'] :\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['A'], dic_locus_lens['B2'])\n",
    "    elif groupe == 'DP':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DPA'], dic_locus_lens['DPB'])\n",
    "    elif groupe == 'DQ':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DQA'], dic_locus_lens['DQB'])\n",
    "    elif groupe == 'DR':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DRA'], dic_locus_lens['DRB'])\n",
    "        \n",
    "    dir_antigen = f'{dir_data}/{antigen}'\n",
    "    df_RMSF = pd.read_csv(f'{dir_antigen}/{antigen}_RMSF.dat', index_col=0, names=['RMSF'], \\\n",
    "                          sep='\\t')\n",
    "    \n",
    "    ix = []\n",
    "    for value in df_RMSF.index:\n",
    "        if (value > 5 and value < len_chain_A-4) or (value > len_chain_A+5 and \\\n",
    "                                                     value < len_chain_A+len_chain_B-4):\n",
    "            ix.append(value)\n",
    "    \n",
    "    df2 = df_RMSF.loc[ix]\n",
    "    df_RSASA = pd.read_csv(f'{dir_antigen}/{surf_meth}_median.txt', header=0, sep='\\t', usecols=[1],\\\n",
    "                          names=['mean'])\n",
    "    df_RSASA.loc[:,'index'] = range(1,len_chain_A+len_chain_B+1)\n",
    "    df_RSASA.set_index('index', inplace=True)\n",
    "    df_RSASA2 = df_RSASA.loc[ix,:]\n",
    "    data = data + list(df2.loc[df_RSASA2['mean'] >= thresh_surf].values)    \n",
    "    \n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(data)\n",
    "    \n",
    "for antigen in list_avail_alleles:\n",
    "    groupe = antigen.split('_')[0] if antigen.split('_')[0] in ['A', 'B', 'C'] else \\\n",
    "                antigen.split('_')[0][:2]\n",
    "    \n",
    "    if groupe in ['A', 'B', 'C'] :\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['A'], dic_locus_lens['B2'])\n",
    "    elif groupe == 'DP':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DPA'], dic_locus_lens['DPB'])\n",
    "    elif groupe == 'DQ':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DQA'], dic_locus_lens['DQB'])\n",
    "    elif groupe == 'DR':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DRA'], dic_locus_lens['DRB'])\n",
    "        \n",
    "    dir_antigen = f'{dir_data}/{antigen}'\n",
    "    df_RMSF = pd.read_csv(f'{dir_antigen}/{antigen}_RMSF.dat', index_col=0, names=['RMSF'], \\\n",
    "                          sep='\\t')\n",
    "    df_RSASA = pd.read_csv(f'{dir_antigen}/{surf_meth}_median.txt', header=0, sep='\\t', usecols=[1],\\\n",
    "                          names=['mean'])\n",
    "    df_RSASA.loc[:,'index'] = range(1,len_chain_A+len_chain_B+1)\n",
    "    df_RSASA.set_index('index', inplace=True)\n",
    "    df2 = df_RMSF.loc[df_RSASA['mean'] >= thresh_surf]\n",
    "    data_std = std_scaler.transform(df2.values.reshape(-1, 1)).reshape(1,-1)[0]\n",
    "    df_std = pd.DataFrame(data_std, columns=['N-RMSF'], index=df2.index)\n",
    "    dic_N_RMSF_all[antigen] = df_std\n",
    "\n",
    "del std_scaler\n",
    "\n",
    "def compute_table_2(index, col):\n",
    "    patch_ID = col['patch_ID']\n",
    "    central_AA = col['central_AA']\n",
    "    antigen = col['antigen']\n",
    "    groupe = antigen.split('_')[0] if antigen.split('_')[0] in ['A', 'B', 'C'] else \\\n",
    "            antigen.split('_')[0][:2]\n",
    "\n",
    "    if groupe in ['A', 'B', 'C'] :\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['A'], dic_locus_lens['B2'])\n",
    "    elif groupe == 'DP':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DPA'], dic_locus_lens['DPB'])\n",
    "    elif groupe == 'DQ':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DQA'], dic_locus_lens['DQB'])\n",
    "    elif groupe == 'DR':\n",
    "        len_chain_A, len_chain_B = (dic_locus_lens['DRA'], dic_locus_lens['DRB'])\n",
    "\n",
    "    dir_antigen = f'{dir_data}/{antigen}'\n",
    "\n",
    "    col_table_2 = pd.Series(name=patch_ID, dtype='object')\n",
    "    col_table_2['patch_ID'] = int(patch_ID)\n",
    "    col_table_2['N_RMSF_central'] = dic_N_RMSF_all[antigen].at[central_AA, 'N-RMSF']\n",
    "    col_table_2['RSASA_avg_central'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID]['RSASA_central_AA'].median()\n",
    "    col_table_2['RSASA_min_central'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID]['RSASA_central_AA'].min()\n",
    "    col_table_2['RSASA_max_central'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID]['RSASA_central_AA'].max()\n",
    "    col_table_2['charge_+_central'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                   patch_ID]['charge_+'].iloc[0]\n",
    "    col_table_2['charge_-_central'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                   patch_ID]['charge_-'].iloc[0]\n",
    "    col_table_2['hydro_central'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                   patch_ID]['hydro'].iloc[0]\n",
    "    list_col_names = table_1.loc[(table_1['patch_ID'] == patch_ID) & \\\n",
    "                              (table_1['frame_ID'] == 0)].T.dropna().index\n",
    "    col_resids = {int(name.split('_')[-1]) for name in list_col_names if 'resid' in name}\n",
    "    df_RSASA = pd.read_csv(f'{dir_antigen}/{surf_meth}_median.txt', header=0, sep='\\t',\\\n",
    "                           usecols=[1], names=['mean'])\n",
    "    df_RSASA.loc[:,'index'] = range(1,len_chain_A+len_chain_B+1)\n",
    "    df_RSASA.set_index('index', inplace=True)\n",
    "    surf_resids = df_RSASA.loc[df_RSASA['mean'] >= thresh_surf].index\n",
    "    list_resid = col_resids.intersection(set(surf_resids))\n",
    "\n",
    "    for resid in list_resid:\n",
    "        col_table_2[f'N_RMSF_resid_{resid}'] = dic_N_RMSF_all[antigen].at[resid, 'N-RMSF']\n",
    "        patch_resid = track_resids_patchs.loc[(track_resids_patchs['antigen'] == antigen) & \\\n",
    "                                                (track_resids_patchs['central_AA'] == resid), \\\n",
    "                                                'patch_ID'].iloc[0]\n",
    "        col_table_2[f'RSASA_avg_resid_{resid}'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID][f'RSASA_resid_{resid}'].median()\n",
    "               \n",
    "        col_table_2[f'RSASA_min_resid_{resid}'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID][f'RSASA_resid_{resid}'].min()\n",
    "        col_table_2[f'RSASA_max_resid_{resid}'] = table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID][f'RSASA_resid_{resid}'].max()\n",
    "        col_table_2[f'charge_+_resid_{resid}'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                    patch_resid]['charge_+'].iloc[0]\n",
    "        col_table_2[f'charge_-_resid_{resid}'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                    patch_resid]['charge_-'].iloc[0]\n",
    "        col_table_2[f'hydro_resid_{resid}'] = table_0.loc[table_0['patch_ID'] == \\\n",
    "                                                    patch_resid]['hydro'].iloc[0]\n",
    "        col_table_2[f'freq_resid_{resid}'] = (table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID][ \\\n",
    "                                                    f'presence_resid_{resid}'].sum()) / \\\n",
    "                                                (table_1.loc[table_1['patch_ID'] == \\\n",
    "                                                    patch_ID][ \\\n",
    "                                                    f'presence_resid_{resid}'].shape[0])\n",
    "    return col_table_2\n",
    "\n",
    "track_resids_patchs = pd.read_csv(f'{dir_data}/track_resids_patchs_table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "table_0 = pd.read_csv(f'{dir_data}/table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "jobs = mp.cpu_count()-2\n",
    "table_2 = pd.DataFrame()\n",
    "\n",
    "for df in list_df:\n",
    "    list_param = []\n",
    "    table_1 = df\n",
    "    df_chunk = track_resids_patchs.loc[track_resids_patchs['patch_ID'].isin(\\\n",
    "                                                            df['patch_ID'].unique())]\n",
    "\n",
    "    for index, col in df_chunk.iterrows():\n",
    "        list_param.append((index, col))\n",
    "\n",
    "    outs_table_2 = Parallel(n_jobs=jobs)(delayed(compute_table_2)(index, col) for index, col in \\\n",
    "                                         list_param)\n",
    "    for obj in outs_table_2:\n",
    "        table_2 = pd.concat([table_2, pd.DataFrame(obj).T], axis=0, ignore_index=True)\n",
    "    \n",
    "    del table_1, df\n",
    "    \n",
    "del list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7022fe2-74cd-4348-b892-0c6ce5378d3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_2.to_csv(f'{dir_data}/table_2_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60798e3-6131-4367-8b21-495a02777165",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "track_resids_patchs = pd.read_csv(f'{dir_data}/track_resids_patchs_table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "table_2 = pd.read_csv(f'{dir_data}/table_2_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29320fa-570f-4b50-9872-d8b3065d7619",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_tuples = []\n",
    "for col_name in table_2.columns:\n",
    "    if 'RSASA_avg_resid' in col_name:\n",
    "        for i in table_2[col_name].dropna().index:\n",
    "            if table_2.at[i, col_name] <= 20:\n",
    "                patch_id = int(table_2.at[i, 'patch_ID'])\n",
    "                antigen = track_resids_patchs[track_resids_patchs['patch_ID'] == patch_id]['antigen'].iloc[0]\n",
    "                central_AA = track_resids_patchs.loc[track_resids_patchs['patch_ID'] == patch_id, 'central_AA'].iloc[0]\n",
    "                print(patch_id, col_name, table_2.at[i, col_name], antigen, central_AA)\n",
    "                list_tuples.append((patch_id, col_name, table_2.at[i, col_name], antigen, central_AA))\n",
    "\n",
    "var = {x[3] for x in list_tuples}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41228985-7113-44ed-8235-f9345f80d34e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Table #3 (PatchAggValues)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d830204-3d10-4d41-b95b-57b3f56c8ae7",
   "metadata": {},
   "source": [
    "Aggregation is performed on table_2* with respect to the AAs in the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f112b-6899-4ad0-9a47-3abd4507a9ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_eplets = pd.read_csv(f'{base_dir}/../data/lists_All_confirmed_eplets.csv', index_col=0)\n",
    "df_all_no_conf_eplets = pd.read_csv(f'{base_dir}/../data/lists_All_non_confirmed_eplets.csv', index_col=0)\n",
    "df_eplets_ext = pd.DataFrame(columns=['list_resids'])\n",
    "df_no_conf_eplets_ext = pd.DataFrame(columns=['list_resids'])\n",
    "track_resids_patchs = pd.read_csv(f'{dir_data}/track_resids_patchs_table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "\n",
    "for allele in list_avail_alleles:\n",
    "    if allele.split('_')[0] in ['DP', 'DQ']:\n",
    "        allele = allele.strip()\n",
    "        locus_A = allele.split('_')[0] if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'A'\n",
    "        locus_B = 'B2' if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'B'\n",
    "                \n",
    "        name_A = allele.split('_')[0]+'A1*'+allele.split('-')[0][3:5]+':'+allele.split('-')[0][5:7]\n",
    "        name_B = allele.split('_')[0]+'B1*'+allele.split('-')[1][0:2]+':'+allele.split('-')[1][2:4]\n",
    "        eplets_A = df_all_eplets[df_all_eplets['allele_name'] == name_A]\n",
    "        eplets_B = df_all_eplets[df_all_eplets['allele_name'] == name_B]\n",
    "        df_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        for index, row in eplets_A.iterrows():\n",
    "            list_conf_eplets = list(filter(None, eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_eplets_ext.loc[name_A, 'list_resids'] = df_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_conf_eplets)))\n",
    "        \n",
    "        df_eplets_ext.loc[name_B, 'list_resids'] = set()\n",
    "        for index, row in eplets_B.iterrows():\n",
    "            list_conf_eplets = list(filter(None, eplets_B.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_eplets_ext.loc[name_B, 'list_resids'] = df_eplets_ext.loc[name_B, 'list_resids'].union(set(map(int, list_conf_eplets)))\n",
    "        \n",
    "        no_conf_eplets_A = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_A]\n",
    "        no_conf_eplets_B = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_B]\n",
    "        \n",
    "        df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        for index, row in no_conf_eplets_A.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = df_no_conf_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "            \n",
    "        df_no_conf_eplets_ext.loc[name_B, 'list_resids'] = set()\n",
    "        for index, row in no_conf_eplets_B.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_B.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_B, 'list_resids'] = df_no_conf_eplets_ext.loc[name_B, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "\n",
    "    else:\n",
    "        allele = allele.strip()\n",
    "        locus_A = allele.split('_')[0] if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'A'\n",
    "        locus_B = 'B2' if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'B'\n",
    "                \n",
    "        name_A = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_A = df_all_eplets[df_all_eplets['allele_name'] == name_A]\n",
    "        df_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        for index, row in eplets_A.iterrows():\n",
    "            df_eplets_ext.loc[name_A, 'list_resids'] = df_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, eplets_A.loc[index, 'eplet_resid_nums'].split(' '))))\n",
    "        \n",
    "        no_conf_eplets_A = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_A]\n",
    "        df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        \n",
    "        for index, row in no_conf_eplets_A.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = df_no_conf_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "\n",
    "def compute_table_3(index, col):\n",
    "    col_table_3 = pd.Series(name=col['patch_ID'], dtype='object')\n",
    "    col_table_3['patch_ID'] = col['patch_ID']\n",
    "    col_table_3['N_RMSF_central'] = col['N_RMSF_central']\n",
    "    resids_patch = {int(name.split('_')[-1]) for name in col.index if 'N_RMSF_resid' in name}\n",
    "    col_table_3['N_RMSF_patch_min'] = min([col[f'N_RMSF_resid_{resid}'] for resid in resids_patch] \\\n",
    "                                          + [col_table_3['N_RMSF_central']])\n",
    "    col_table_3['N_RMSF_patch_max'] = max([col[f'N_RMSF_resid_{resid}'] for resid in resids_patch] \\\n",
    "                                          + [col_table_3['N_RMSF_central']])\n",
    "    col_table_3['N_RMSF_patch_avg'] = np.average([col[f'N_RMSF_resid_{resid}'] \\\n",
    "                                                  for resid in resids_patch] \\\n",
    "                                                 + [col_table_3['N_RMSF_central']])\n",
    "    col_table_3['N_RMSF_patch_avg_freq'] = np.average([col[f'N_RMSF_resid_{resid}']*\\\n",
    "                                                       col[f'freq_resid_{resid}'] \\\n",
    "                                                     for resid in resids_patch] \\\n",
    "                                                      + [col_table_3['N_RMSF_central']])\n",
    "    col_table_3['RSASA_min_central'] = col['RSASA_min_central']\n",
    "    col_table_3['RSASA_max_central'] = col['RSASA_max_central']\n",
    "    col_table_3['RSASA_avg_central'] = col['RSASA_avg_central']\n",
    "    col_table_3['charge_pos_central'] = col['charge_+_central']\n",
    "    col_table_3['charge_neg_central'] = col['charge_-_central']\n",
    "    col_table_3['hydro_central'] = col['hydro_central']\n",
    "    col_table_3['charge_patch_pos_freq'] = sum([col[f'charge_+_resid_{resid}']*\\\n",
    "                                                col[f'freq_resid_{resid}'] \\\n",
    "                                               for resid in resids_patch] \\\n",
    "                                                  + [col_table_3['charge_pos_central']])\n",
    "    col_table_3['charge_patch_neg_freq '] = sum([col[f'charge_-_resid_{resid}']*\\\n",
    "                                                 col[f'freq_resid_{resid}'] \\\n",
    "                                               for resid in resids_patch] \\\n",
    "                                                    + [col_table_3['charge_neg_central']])\n",
    "    col_table_3['charge_patch_pos'] = sum([col[f'charge_+_resid_{resid}'] \\\n",
    "                                           for resid in resids_patch] \\\n",
    "                                              + [col_table_3['charge_pos_central']])\n",
    "    col_table_3['charge_patch_neg'] = sum([col[f'charge_-_resid_{resid}'] \\\n",
    "                                           for resid in resids_patch] \\\n",
    "                                          + [col_table_3['charge_neg_central']])\n",
    "    col_table_3['RSASA_patch_min_avg_freq']= np.average([col[f'RSASA_min_resid_{resid}']*\\\n",
    "                                                         col[f'freq_resid_{resid}'] \\\n",
    "                                                        for resid in resids_patch] \\\n",
    "                                                        + [col_table_3['RSASA_min_central']])\n",
    "    col_table_3['RSASA_patch_max_avg_freq'] = np.average([col[f'RSASA_max_resid_{resid}']*\\\n",
    "                                                          col[f'freq_resid_{resid}'] \\\n",
    "                                                      for resid in resids_patch] \\\n",
    "                                                         + [col_table_3['RSASA_max_central']])\n",
    "    col_table_3['RSASA_patch_avg_avg_freq'] = np.average([col[f'RSASA_avg_resid_{resid}']*\\\n",
    "                                                          col[f'freq_resid_{resid}'] \\\n",
    "                                                      for resid in resids_patch] \\\n",
    "                                                         + [col_table_3['RSASA_avg_central']])\n",
    "    col_table_3['RSASA_patch_min_avg'] = np.average([col[f'RSASA_min_resid_{resid}'] \\\n",
    "                                                     for resid in resids_patch] \\\n",
    "                                                      + [col_table_3['RSASA_min_central']])\n",
    "    col_table_3['RSASA_patch_max_avg'] = np.average([col[f'RSASA_max_resid_{resid}'] \\\n",
    "                                                     for resid in resids_patch] \\\n",
    "                                                      + [col_table_3['RSASA_max_central']])\n",
    "    col_table_3['RSASA_patch_avg_avg'] = np.average([col[f'RSASA_avg_resid_{resid}'] \\\n",
    "                                                     for resid in resids_patch] \\\n",
    "                                                      + [col_table_3['RSASA_avg_central']])\n",
    "    col_table_3['hydro_patch_min'] = min([col[f'hydro_resid_{resid}'] for resid in resids_patch] \\\n",
    "                                         + [col_table_3['hydro_central']])\n",
    "    col_table_3['hydro_patch_max'] = max([col[f'hydro_resid_{resid}'] for resid in resids_patch] \\\n",
    "                                         + [col_table_3['hydro_central']])\n",
    "    col_table_3['hydro_patch_avg'] = np.average([col[f'hydro_resid_{resid}'] \\\n",
    "                                                 for resid in resids_patch] \\\n",
    "                                                    + [col_table_3['hydro_central']])\n",
    "    col_table_3['hydro_patch_avg_freq'] = np.average([col[f'hydro_resid_{resid}']*\\\n",
    "                                                      col[f'freq_resid_{resid}']\\\n",
    "                                                       for resid in resids_patch] \\\n",
    "                                                         + [col_table_3['hydro_central']])\n",
    "    \n",
    "    allele = track_resids_patchs[track_resids_patchs['patch_ID'] == \\\n",
    "                                  col['patch_ID']]['antigen'].iloc[0]\n",
    "    central_AA = track_resids_patchs[track_resids_patchs['patch_ID'] == \\\n",
    "                                  col['patch_ID']]['central_AA'].iloc[0]\n",
    "    if allele.split('_')[0] in ['DP', 'DQ']:\n",
    "        allele = allele.strip()\n",
    "        name_A = allele.split('_')[0]+'A1*'+allele.split('-')[0][3:5]+':'+allele.split('-')[0][5:7]\n",
    "        name_B = allele.split('_')[0]+'B1*'+allele.split('-')[1][0:2]+':'+allele.split('-')[1][2:4]\n",
    "        eplets_A_ext = df_eplets_ext.loc[name_A, 'list_resids']\n",
    "        eplets_B_ext = df_eplets_ext.loc[name_B, 'list_resids']\n",
    "        eplets_B_ext = set(map(lambda x:x+dic_locus_lens[allele.split('_')[0]+'A'], eplets_B_ext))\n",
    "        eplets_allele = eplets_A_ext.union(eplets_B_ext)\n",
    "        \n",
    "    if allele.split('_')[0] in ['A', 'B', 'C']:\n",
    "        allele = allele.strip()\n",
    "        name_A = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_A_ext = df_eplets_ext.loc[name_A, 'list_resids']\n",
    "        eplets_allele = eplets_A_ext\n",
    "        \n",
    "    if allele.split('_')[0][:2] in 'DR':\n",
    "        allele = allele.strip()\n",
    "        name_B = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_B_ext = df_eplets_ext.loc[name_B, 'list_resids']\n",
    "        eplets_B_ext = set(map(lambda x:x+dic_locus_lens['DRA'], eplets_B_ext))\n",
    "        eplets_allele = eplets_B_ext\n",
    "        \n",
    "    col_table_3['class'] = 1 if central_AA in eplets_allele else 0\n",
    "    \n",
    "    return col_table_3\n",
    "    \n",
    "table_2 = pd.read_csv(f'{dir_data}/table_2_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', \\\n",
    "                      sep=',', index_col=0)\n",
    "jobs = mp.cpu_count()-2\n",
    "\n",
    "list_param = []\n",
    "for index, col in table_2.iterrows():\n",
    "    list_param.append((index, col.dropna()))\n",
    "\n",
    "outs_table_3 = Parallel(n_jobs=jobs)(delayed(compute_table_3)(index, col) for index, col in \\\n",
    "                                         list_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720e6ed-fa19-4b5c-b4cf-10dc70ec2107",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_3 = pd.DataFrame()\n",
    "for obj in outs_table_3:\n",
    "    table_3 = pd.concat([table_3, pd.DataFrame(obj).T], axis=0, ignore_index=True)\n",
    "\n",
    "table_3['patch_ID'] = list(map(int, table_3['patch_ID']))\n",
    "table_3['class'] = list(map(int, table_3['class']))\n",
    "table_3.to_csv(f'{dir_data}/table_3_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "N-RMSF",
   "language": "python",
   "name": "n-rmsf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
