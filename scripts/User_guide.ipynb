{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **User guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the procedure to follow in order to generate and use the descriptors defined in HLA-EpiCheck. This procedure could be used to process new antigen data (new MD trajectories) or to reproduce the results obtained in the paper. It is assumed that each antigen has a directory and that this directory contains the DCD and PSF files of the MD trajectory of each antigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run MD simulations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get an initial structure for each antigen to process (from PDB or Alphafold).\n",
    "\n",
    "2. Generate the psf file of each structure using VMD.\n",
    "\n",
    "3. Solvate and ionize each structure using VMD.\n",
    "\n",
    "4. Modify the relevant fields/parameters in the NAMD configuration files 'general_min.conf' and 'general_MD.conf' (see the data folder).\n",
    "\n",
    "5. Run the minimization and MD simulation using NAMD.\n",
    "\n",
    "6. Clean the MD traejctories (remote waters and keep the relevant frames).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generation of the descriptors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare a directory called \"PDBs\" inside each antigen's directory. This directory will contain the set of PDB files that represent the MD trajectory of the antigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute SASA and RSASA values using the script 'RSASA_trajectory.py':\n",
    "\n",
    "    USAGE : python RSASA_trajectory.py <data_directory>\n",
    "\n",
    "    <data_directory> corresponds to the absolute path to  the antigen directories which contain the data to be processed.\n",
    "\n",
    "    Run the script from the same directory where it is located.\n",
    "\n",
    "    Attention! This script requires the python package 'VMD-PYTHON'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute RMSF values using the script 'run_batch_RMSF.sh':\n",
    "\n",
    "    USAGE: ./run_batch_RMSF.sh  <number of frames> <data_directory> \n",
    "    \n",
    "    <number of frames> corresponds to the number of frames to process.\n",
    "    \n",
    "    <data_directory> corresponds to the absolute path to the antigen directories which contain the data to be processed.\n",
    "    \n",
    "    Run the script from the same directory where it is located.\n",
    "    \n",
    "    This script will generate the RMSF data for each antigen on files like this : <antigen name>_RMSF.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the pre-patches using the script 'compute_prepatches.tcl'. We recommend using 'gnu parallel' to distribute the calculations across several nodes (command used in a OAR cluster):\n",
    "\n",
    "    Run in a bash shell: \n",
    "\n",
    "    ```\n",
    "    dir_data=<data_directory>\n",
    "\n",
    "    nohup parallel --joblog $PWD/parallel.log --progress -j 60 --ssh oarsh --sshloginfile $OAR_NODEFILE vmd -dispdev text -e $PWD/compute_patches.tcl -args $dir_data/{1}/PDBs 15  ::: $(while read line; do antigen=${line%/} && antigen=${antigen##*/} && echo $antigen; done < $dir_data/list_antigens.txt) > vmd_patches_out.log 2> vmd_patches_err.log &\n",
    "\n",
    "    ```\n",
    "\n",
    "    <data_directory> corresponds to the absolute path to the antigen directories which contain the data to be processed.\n",
    "    \n",
    "    For more information about 'compute_prepatches.tcl', please see the comments in the script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run the 'dataset_gen_radius_15.ipynb' notebook:\n",
    "\n",
    "    Please pay attention to the required packages in the 'Imports' section.\n",
    "\n",
    "    The variables 'radius_patch' and 'thresh_surf' in the  'Definitions' section set the patch size and the solven-accessibility threshold parameters respectively. \n",
    "\n",
    "    Please modify the 'dir_data' variable in the 'Definitions' section. This variable define the absolute path to the antigen directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training (just for reproducibility purpose, not necessary for making predictions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applying the patch filter (removing patches according to the inclusion criteria described in the article)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please modify the 'dir_data' variable in the first cell of this section. This variable define the absolute path to the antigen directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def convert_name(name):\n",
    "    if name.split('_')[0] in ['A', 'B', 'C']:\n",
    "        new_name = f\"{name.split('_')[0]}*{name.split('_')[1][:2]}:{name.split('_')[1][2:4]}\"\n",
    "\n",
    "    elif (len(name.split('_')[0]) > 2) and (name.split('_')[0][:3] == 'DQB'):\n",
    "        new_name = f\"DQB1*{name.split('_')[1][:2]}:{name.split('_')[1][2:4]}-\" + \\\n",
    "                    f\"DQA1*{name.split('_')[2][:2]}:{name.split('_')[2][2:4]}\"\n",
    "\n",
    "    elif (len(name.split('_')[0]) > 2) and (name.split('_')[0][:3] == 'DRB'):\n",
    "        new_name = f\"{name.split('_')[0]}*{name.split('_')[1][:2]}:{name.split('_')[1][2:4]}\"\n",
    "\n",
    "    elif name.split('_')[0] in ['DP', 'DQ']:\n",
    "        new_name = f\"{name.split('_')[0]}B1*{name.split('_')[1][5:7]}:{name.split('_')[1][7:9]}-\" + \\\n",
    "                    f\"{name.split('_')[0]}A1*{name.split('_')[1][:2]}:{name.split('_')[1][2:4]}\"\n",
    "    return new_name\n",
    "\n",
    "def formal2informal_allele(name):\n",
    "    if name.split('*')[0] in ['A', 'B', 'C']:\n",
    "        groupe = name.split('*')[0]\n",
    "        new_name = f\"{groupe}_{name.split('*')[1][:2]}{name.split(':')[1][:2]}\"\n",
    "    elif name.split('*')[0][:2] in ['DP', 'DQ']:\n",
    "        groupe = name.split('*')[0][:2]\n",
    "        new_name = f\"{name.split('*')[1][:2]}{name.split(':')[1][:2]}\"\n",
    "    elif name.split('*')[0][:2] in ['DR']:\n",
    "        groupe = name.split('*')[0]\n",
    "        new_name = f\"{groupe}_{name.split('*')[1][:2]}{name.split(':')[1][:2]}\"\n",
    "    return new_name\n",
    "\n",
    "def formal2informal_antigen(name):\n",
    "    if name.split('*')[0] in ['A', 'B', 'C']:\n",
    "        groupe = name.split('*')[0]\n",
    "        new_name = f\"{groupe}_{name.split('*')[1][:2]}{name.split(':')[1][:2]}\"\n",
    "    elif name.split('*')[0][:2] in ['DP', 'DQ']:\n",
    "        groupe = name.split('*')[0][:2]\n",
    "        new_name = f\"{groupe}_{name.split('-')[1][5:7]}{name.split('-')[1][8:10]}-{name.split('*')[1][:2]}{name.split('*')[1][3:5]}\"\n",
    "    elif name.split('*')[0][:2] in ['DR']:\n",
    "        groupe = name.split('*')[0]\n",
    "        new_name = f\"{groupe}_{name.split('*')[1][:2]}{name.split(':')[1][:2]}\"\n",
    "    return new_name\n",
    "\n",
    "dic_hydro_KD = {\n",
    "                'G': -0.4,\n",
    "                'A': 1.8,\n",
    "                'V': 4.2,\n",
    "                'F': 2.8,\n",
    "                'P': -1.6,\n",
    "                'M': 1.9,\n",
    "                'I': 4.5,\n",
    "                'L': 3.8,\n",
    "                'D': -3.5,\n",
    "                'E': -3.5,\n",
    "                'K': -3.9,\n",
    "                'R': -4.5,\n",
    "                'S': -0.8,\n",
    "                'T': -0.7,\n",
    "                'Y': -1.3,\n",
    "                'H': -3.2,\n",
    "                'C': 2.5,\n",
    "                'N': -3.5,\n",
    "                'Q': -3.5,\n",
    "                'W': -0.9\n",
    "}\n",
    "\n",
    "dic_hydro_E = {\n",
    "                'G': 0.48,\n",
    "                'A': 0.62,\n",
    "                'V': 1.08,\n",
    "                'F': 1.19,\n",
    "                'P': 0.12,\n",
    "                'M': 0.64,\n",
    "                'I': 1.38,\n",
    "                'L': 1.06,\n",
    "                'D': -0.9,\n",
    "                'E': -0.74,\n",
    "                'K': -1.5,\n",
    "                'R': -2.53,\n",
    "                'S': -0.18,\n",
    "                'T': -0.05,\n",
    "                'Y': 0.26,\n",
    "                'H': -0.4,\n",
    "                'C': 0.29,\n",
    "                'N': -0.78,\n",
    "                'Q': -0.85,\n",
    "                'W': 0.81\n",
    "}\n",
    "\n",
    "dic_charge = {\n",
    "                'G': 0,\n",
    "                'A': 0,\n",
    "                'V': 0,\n",
    "                'F': 0,\n",
    "                'P': 0,\n",
    "                'M': 0,\n",
    "                'I': 0,\n",
    "                'L': 0,\n",
    "                'D': -1,\n",
    "                'E': -1,\n",
    "                'K': 1,\n",
    "                'R': 1,\n",
    "                'S': 0,\n",
    "                'T': 0,\n",
    "                'Y': 0,\n",
    "                'H': 0,  \n",
    "                'C': 0,\n",
    "                'N': 0,\n",
    "                'Q': 0,\n",
    "                'W': 0, \n",
    "}\n",
    "\n",
    "\n",
    "dic_groupe_lens = {'A':375, 'B':375, 'C':375, 'DP':374, 'DQ':378, 'DR':374}\n",
    "dic_locus_lens = {'A':276, 'B':276, 'C':276, 'B2':99, 'DQA':186, 'DQB':192, 'DRA':182, 'DRB':192,\n",
    "                   'DPA':183, 'DPB':191}\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "pdb2fasta = f'{base_dir}/pdb2fasta.sh' \n",
    "surf_meth = 'RSASA'\n",
    "hydro_scale = 'KD'\n",
    "\n",
    "dir_data='/home/damaya/capsid_new/HLA-EpiCheck'\n",
    "\n",
    "radius_patch = 15\n",
    "thresh_surf = 20\n",
    "\n",
    "list_avail_alleles = []\n",
    "with open(f'{dir_data}/list_antigens.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        list_avail_alleles.append(line.strip().split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3_5 = pd.read_csv(f'{dir_data}/table_3_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "\n",
    "dir_eplets = f'{base_dir}/../data'\n",
    "df_all_eplets = pd.read_csv(f'{dir_eplets}/lists_All_confirmed_eplets.csv', index_col=0)\n",
    "df_all_no_conf_eplets = pd.read_csv(f'{dir_eplets}/lists_All_non_confirmed_eplets.csv', index_col=0)\n",
    "df_eplets_ext = pd.DataFrame(columns=['list_resids'])\n",
    "df_no_conf_eplets_ext = pd.DataFrame(columns=['list_resids'])\n",
    "track_resids_patchs = pd.read_csv(f'{dir_data}/track_resids_patchs_table_0_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "table_2 = pd.read_csv(f'{dir_data}/table_2_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', sep=',', index_col=0)\n",
    "\n",
    "for allele in list_avail_alleles:\n",
    "    if allele.split('_')[0] in ['DP', 'DQ']:\n",
    "        allele = allele.strip()\n",
    "        locus_A = allele.split('_')[0] if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'A'\n",
    "        locus_B = 'B2' if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'B'\n",
    "                \n",
    "        name_A = allele.split('_')[0]+'A1*'+allele.split('-')[0][3:5]+':'+allele.split('-')[0][5:7]\n",
    "        name_B = allele.split('_')[0]+'B1*'+allele.split('-')[1][0:2]+':'+allele.split('-')[1][2:4]\n",
    "        eplets_A = df_all_eplets[df_all_eplets['allele_name'] == name_A]\n",
    "        eplets_B = df_all_eplets[df_all_eplets['allele_name'] == name_B]\n",
    "        df_eplets_ext.loc[name_A, 'list_resids'] = set() \n",
    "        for index, row in eplets_A.iterrows():\n",
    "            list_conf_eplets = list(filter(None, eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_eplets_ext.loc[name_A, 'list_resids'] = df_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_conf_eplets)))\n",
    "        \n",
    "        df_eplets_ext.loc[name_B, 'list_resids'] = set()\n",
    "        for index, row in eplets_B.iterrows():\n",
    "            list_conf_eplets = list(filter(None, eplets_B.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_eplets_ext.loc[name_B, 'list_resids'] = df_eplets_ext.loc[name_B, 'list_resids'].union(set(map(int, list_conf_eplets)))\n",
    "        \n",
    "        no_conf_eplets_A = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_A]\n",
    "        no_conf_eplets_B = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_B]\n",
    "        \n",
    "        df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        for index, row in no_conf_eplets_A.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = df_no_conf_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "            \n",
    "        df_no_conf_eplets_ext.loc[name_B, 'list_resids'] = set()\n",
    "        for index, row in no_conf_eplets_B.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_B.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_B, 'list_resids'] = df_no_conf_eplets_ext.loc[name_B, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "\n",
    "    else:\n",
    "        allele = allele.strip()\n",
    "        locus_A = allele.split('_')[0] if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'A'\n",
    "        locus_B = 'B2' if allele.split('_')[0] in ['A', 'B', 'C'] else allele.split('_')[0][:2]+'B'\n",
    "                \n",
    "        name_A = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_A = df_all_eplets[df_all_eplets['allele_name'] == name_A]\n",
    "        df_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        for index, row in eplets_A.iterrows():\n",
    "            df_eplets_ext.loc[name_A, 'list_resids'] = df_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, eplets_A.loc[index, 'eplet_resid_nums'].split(' '))))\n",
    "        \n",
    "        no_conf_eplets_A = df_all_no_conf_eplets[df_all_no_conf_eplets['allele_name'] == name_A]\n",
    "        df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = set()\n",
    "        \n",
    "        for index, row in no_conf_eplets_A.iterrows():\n",
    "            list_no_conf_eplets = list(filter(None, no_conf_eplets_A.loc[index, 'eplet_resid_nums'].split(' ')))\n",
    "            df_no_conf_eplets_ext.loc[name_A, 'list_resids'] = df_no_conf_eplets_ext.loc[name_A, 'list_resids'].union(set(map(int, list_no_conf_eplets)))\n",
    "\n",
    "df_seqs = pd.DataFrame()\n",
    "for locus in ['A', 'B', 'C', 'DPA1', 'DPB1', 'DQA1', 'DQB1', 'DRA', 'DRB']:\n",
    "    df = pd.read_csv(f'{base_dir}/../data/{locus}_nr_results.txt', sep='\\t', header=None)\n",
    "    df_seqs = pd.concat([df_seqs, df], axis=0, ignore_index=True)\n",
    "\n",
    "for index, col in df_seqs.iterrows():\n",
    "    antigen = df_seqs.at[index, 0].split('|')[0]\n",
    "    locus = antigen.split('*')[0].strip()\n",
    "    antigen = f'{locus}_{antigen.split(\":\")[0].split(\"*\")[1]}{antigen.split(\":\")[1]}'\n",
    "    df_seqs.at[index, 0] =  antigen\n",
    "\n",
    "conserved_pos = {'A':[], 'B':[], 'C':[], 'DP':[], 'DQ':[], 'DRB':[]}\n",
    "dic_eplet_cons = {'A':set(), 'B':set(), 'C':set(), 'DP':set(), 'DQ':set(), 'DRB':set()}\n",
    "distri_test = []   \n",
    "for locus in ['A', 'B', 'C', 'DP', 'DQ', 'DRB']:\n",
    "    if locus in ['A', 'B', 'C']:\n",
    "        antigens_simu = [x for x in list_avail_alleles if locus == x.split('_')[0]]\n",
    "        \n",
    "        for pos in range(dic_locus_lens[locus]):\n",
    "            var_antigens = set()\n",
    "            polym = set()\n",
    "            for index, col in df_seqs[df_seqs[0].isin(antigens_simu)].iterrows():\n",
    "                polym.add(df_seqs.at[index, 1][pos])\n",
    "                var_antigens.add(df_seqs.at[index, 0])\n",
    "                \n",
    "            if len(polym) == 1: conserved_pos[locus].append(pos+1)\n",
    "        conserved_pos[locus] = conserved_pos[locus] + [x for x in range(277,376)]\n",
    "        \n",
    "        \n",
    "    if locus in ['DP', 'DQ']:\n",
    "        antigens_simu = [x for x in list_avail_alleles if locus == x.split('_')[0]]\n",
    "        antigens_simu_A = [f'{locus}A1_{x.split(\"_\")[1][:4]}' for x in antigens_simu]\n",
    "        antigens_simu_B = [f'{locus}B1_{x.split(\"-\")[1]}' for x in antigens_simu]\n",
    "\n",
    "        for pos in range(dic_locus_lens[locus+'A']):\n",
    "            var_antigens = set()\n",
    "            polym = set()\n",
    "            for index, col in df_seqs[df_seqs[0].isin(antigens_simu_A)].iterrows():\n",
    "                polym.add(df_seqs.at[index, 1][pos])\n",
    "                var_antigens.add(df_seqs.at[index, 0])\n",
    "                \n",
    "            if len(polym) == 1: conserved_pos[locus].append(pos+1)\n",
    "        \n",
    "        for pos in range(dic_locus_lens[locus+'B']):\n",
    "            var_antigens = set()\n",
    "            polym = set()\n",
    "            for index, col in df_seqs[df_seqs[0].isin(antigens_simu_B)].iterrows():\n",
    "                polym.add(df_seqs.at[index, 1][pos])\n",
    "                var_antigens.add(df_seqs.at[index, 0])\n",
    "                \n",
    "            if len(polym) == 1: conserved_pos[locus].append(dic_locus_lens[locus+'A'] \\\n",
    "                                                            + pos + 1)\n",
    "        \n",
    "    if locus in ['DRB']:\n",
    "        antigens_simu = [x for x in list_avail_alleles if locus in x.split('_')[0]]\n",
    "        \n",
    "        for pos in range(dic_locus_lens['DRB']):\n",
    "            var_antigens = set()\n",
    "            polym = set()\n",
    "            for index, col in df_seqs[df_seqs[0].isin(antigens_simu)].iterrows():\n",
    "                polym.add(df_seqs.at[index, 1][pos])\n",
    "                var_antigens.add(df_seqs.at[index, 0])\n",
    "                \n",
    "            if len(polym) == 1: conserved_pos[locus].append(dic_locus_lens['DRA'] \\\n",
    "                                                            + pos + 1)\n",
    "        conserved_pos[locus] = conserved_pos[locus] + [x for x in range(1,183)]\n",
    "            \n",
    "for index, col in table_3_5[table_3_5['class'] == 0].iterrows():\n",
    "    allele = track_resids_patchs[track_resids_patchs['patch_ID'] == \\\n",
    "                                  col['patch_ID']]['antigen'].iloc[0]\n",
    "    central_AA = track_resids_patchs[track_resids_patchs['patch_ID'] == \\\n",
    "                                  col['patch_ID']]['central_AA'].iloc[0]\n",
    "    patch_ID = col['patch_ID']\n",
    "    col_names = table_2[table_2['patch_ID'] == patch_ID].T.dropna().index\n",
    "    resids_patch = {int(name.split('_')[-1]) for name in col_names if 'N_RMSF_resid' in name}\n",
    "    \n",
    "    if allele.split('_')[0] in ['DP', 'DQ']:\n",
    "        allele = allele.strip()\n",
    "        name_A = allele.split('_')[0]+'A1*'+allele.split('-')[0][3:5]+':'+allele.split('-')[0][5:7]\n",
    "        name_B = allele.split('_')[0]+'B1*'+allele.split('-')[1][0:2]+':'+allele.split('-')[1][2:4]\n",
    "        eplets_A_ext = df_eplets_ext.loc[name_A, 'list_resids']\n",
    "        eplets_B_ext = df_eplets_ext.loc[name_B, 'list_resids']\n",
    "        eplets_B_ext = set(map(lambda x:x+dic_locus_lens[allele.split('_')[0]+'A'], eplets_B_ext))\n",
    "        non_conf_eplets_A = df_no_conf_eplets_ext.loc[name_A, 'list_resids']\n",
    "        non_conf_eplets_B = df_no_conf_eplets_ext.loc[name_B, 'list_resids']\n",
    "        non_conf_eplets_B = set(map(lambda x:x+dic_locus_lens[allele.split('_')[0]+'A'], \\\n",
    "                                    non_conf_eplets_B))\n",
    "        conf_eplets = eplets_A_ext.union(eplets_B_ext)\n",
    "        non_conf_eplets = non_conf_eplets_A.union(non_conf_eplets_B)\n",
    "        eplets_allele = conf_eplets.union(non_conf_eplets)\n",
    "        \n",
    "    if allele.split('_')[0] in ['A', 'B', 'C']:\n",
    "        allele = allele.strip()\n",
    "        name_A = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_A_ext = df_eplets_ext.loc[name_A, 'list_resids']\n",
    "        non_conf_eplets_A = df_no_conf_eplets_ext.loc[name_A, 'list_resids']\n",
    "        eplets_allele = eplets_A_ext.union(non_conf_eplets_A)\n",
    "        \n",
    "    if allele.split('_')[0][:2] in 'DR':\n",
    "        allele = allele.strip()\n",
    "        name_B = allele.split('_')[0]+'*'+allele.split('_')[1][0:2]+':'+allele.split('_')[1][2:4]\n",
    "        eplets_B_ext = df_eplets_ext.loc[name_B, 'list_resids']\n",
    "        eplets_B_ext = set(map(lambda x:x+dic_locus_lens['DRA'], eplets_B_ext))\n",
    "        non_conf_eplets_B = df_no_conf_eplets_ext.loc[name_B, 'list_resids']\n",
    "        non_conf_eplets_B = set(map(lambda x:x+dic_locus_lens['DRA'], \\\n",
    "                                    non_conf_eplets_B))\n",
    "        eplets_allele = eplets_B_ext.union(non_conf_eplets_B)\n",
    "    \n",
    "    resids_patch.add(central_AA)\n",
    "    groupe = allele.split('_')[0] if len(allele.split('_')[0]) < 3 else 'DRB'\n",
    "    distri_test.append(len(resids_patch))\n",
    "    dic_eplet_cons[groupe] = dic_eplet_cons[groupe].union(eplets_allele.intersection(set(conserved_pos[groupe])))\n",
    "    if (len(resids_patch.intersection(eplets_allele)) != 0) or \\\n",
    "        (len(resids_patch.intersection(set(conserved_pos[groupe]))) == len(resids_patch)) :\n",
    "        table_3_5.drop(index, axis=0, inplace=True)\n",
    "\n",
    "table_3_5.to_csv(f'{dir_data}/table_3_5_{surf_meth}_{thresh_surf}_hydro_{hydro_scale}_radius_{radius_patch}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Redundancy reduction (90% identity)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ag = pd.DataFrame(columns=['patch_ID', 'antigen',  'class'])\n",
    "\n",
    "for ix, col in table_3_5.iterrows():\n",
    "    antigen = track_resids_patchs[track_resids_patchs['patch_ID'] == col['patch_ID']]['antigen'].iloc[0]\n",
    "    df_ag.loc[len(df_ag)] = [col['patch_ID'], convert_name(antigen), col['class']]\n",
    "\n",
    "df_clusters = pd.read_csv(f'{base_dir}/../data/DB_clu_0.9_all.tsv', sep='\\t', index_col=None, names=['representative', 'member'])\n",
    "dic_non_epitope = {i:(0, '') for i in df_clusters['representative'].unique()}\n",
    "dic_clusters = {i:[] for i in df_clusters['representative'].unique()}\n",
    "dic_clusters_antigens = {i:[] for i in df_clusters['representative'].unique()}\n",
    "df_ag2 = df_ag.groupby(['antigen', 'class'])['antigen'].agg(['count'])\n",
    "trace = []\n",
    "for repre in df_clusters['representative'].unique():\n",
    "    df_members = df_clusters[df_clusters['representative'] == repre]\n",
    "    for member in df_members['member'].unique():\n",
    "        dic_clusters[repre].append(member)\n",
    "        if (len(member.split('*')[0]) > 1 ) and (member.split('*')[0][:2] in ['DQ',  'DP']):\n",
    "            list_antigens = [i for i in df_ag['antigen'].unique() if member in i]\n",
    "            for antigen in list_antigens:\n",
    "                dic_clusters_antigens[repre].append(antigen)\n",
    "                n_non_eplet = df_ag2.loc[(antigen, 0)].iloc[0] if (antigen, 0) in df_ag2.index else 0\n",
    "                cond_1 = repre in ['DPB1*10:01'] and antigen in ['DPB1*28:01-DPA1*01:05']\n",
    "                cond_2 = repre in ['DQB1*03:03'] and antigen in ['DQB1*02:01-DQA1*05:08']\n",
    "                cond_3 = repre in ['DQB1*03:02'] and antigen in ['DQB1*06:09-DQA1*01:02']\n",
    "                cond_4 = repre in ['DPB1*15:01'] and antigen in ['DPB1*28:01-DPA1*01:05']\n",
    "                cond_5 = repre in ['DQA1*01:02'] and antigen in ['DQB1*06:09-DQA1*01:02']\n",
    "                if not (cond_1 or cond_2 or  cond_3 or cond_4 or cond_5):\n",
    "                    if dic_non_epitope[repre][0] < n_non_eplet:\n",
    "                        dic_non_epitope[repre] = (n_non_eplet, antigen)\n",
    "\n",
    "        else: \n",
    "            dic_clusters_antigens[repre].append(member)\n",
    "            n_non_eplet = df_ag2.loc[(member, 0)].iloc[0] if (member, 0) in df_ag2.index else 0\n",
    "            if dic_non_epitope[repre][0] < n_non_eplet:\n",
    "                dic_non_epitope[repre] = (n_non_eplet, member)\n",
    "\n",
    "\n",
    "df_all_eplets = pd.read_csv(f'{base_dir}/../data/All_confirmed_eplets.csv', index_col=None, usecols=[ 'eplet_name', 'eplet_residues', 'allele_name'])\n",
    "\n",
    "df_nr_dataset = pd.DataFrame(columns=['locus', 'eplet_name', 'resid', 'antigen', 'patch_ID'])\n",
    "for cluster in dic_non_epitope.keys():\n",
    "    if (len(cluster.split('*')[0]) > 1 ) and (cluster.split('*')[0][:2] in ['DQ',  'DP']):\n",
    "        locus = cluster.split('*')[0][:2]\n",
    "        list_alleles = dic_non_epitope[cluster][1].split('-')\n",
    "\n",
    "        for allele in list_alleles:\n",
    "            df = df_all_eplets[df_all_eplets['allele_name'] == allele]\n",
    "        \n",
    "            for ix, col in df.iterrows():\n",
    "                list_resids = [i for i in df.loc[ix, 'eplet_residues'].split('(') if not ')' in i]\n",
    "                list_resids = list_resids[0].strip().split(' ')\n",
    "                list_resid_num = []\n",
    "                for resid in list_resids:\n",
    "                    resid_num = ''\n",
    "                    for i in resid:\n",
    "                        if i.isdigit():\n",
    "                            resid_num += i\n",
    "                        else:\n",
    "                            resid_name = i\n",
    "                            break\n",
    "                    resid_num = int(resid_num) if locus+'A' in allele.split('*')[0] else int(resid_num)+dic_locus_lens[locus+'A']\n",
    "                    patch = track_resids_patchs.loc[(track_resids_patchs['antigen'] == formal2informal_antigen(dic_non_epitope[cluster][1])) & \\\n",
    "                            (track_resids_patchs['central_AA'] == resid_num), 'patch_ID']\n",
    "                    patch = patch.iloc[0] if len(patch) > 0 else float('nan')\n",
    "                    df_nr_dataset.loc[len(df_nr_dataset)] = [locus, df.loc[ix, 'eplet_name'], resid_num,\n",
    "                                                            dic_non_epitope[cluster][1], patch]\n",
    "\n",
    "    else:\n",
    "        locus = cluster.split('*')[0][:2] if len(cluster.split('*')[0]) > 1 else cluster.split('*')[0]\n",
    "        df = df_all_eplets[df_all_eplets['allele_name'] == dic_non_epitope[cluster][1]]\n",
    "        \n",
    "        for ix, col in df.iterrows():\n",
    "            list_resids = [i for i in df.loc[ix, 'eplet_residues'].split('(') if not ')' in i]\n",
    "            list_resids = list_resids[0].strip().split(' ')\n",
    "            list_resid_num = []\n",
    "            for resid in list_resids:\n",
    "                resid_num = ''\n",
    "                for i in resid:\n",
    "                    if i.isdigit():\n",
    "                        resid_num += i\n",
    "                    else:\n",
    "                        resid_name = i\n",
    "                        break\n",
    "                resid_num = int(resid_num) if locus in ['A', 'B', 'C'] else int(resid_num)+dic_locus_lens['DRA']\n",
    "                antigen = dic_non_epitope[cluster][1]\n",
    "                patch = track_resids_patchs.loc[(track_resids_patchs['antigen'] == formal2informal_allele(antigen)) & \\\n",
    "                        (track_resids_patchs['central_AA'] == int(resid_num)), 'patch_ID']\n",
    "                patch = patch.iloc[0] if len(patch) > 0 else float('nan')\n",
    "                df_nr_dataset.loc[len(df_nr_dataset)] = [locus, df.loc[ix, 'eplet_name'], int(resid_num),\n",
    "                                                        antigen, patch]\n",
    "\n",
    "df_nr_dataset = df_nr_dataset.dropna()\n",
    "df_nr_dataset.index = range(len(df_nr_dataset))\n",
    "\n",
    "dic_epitopes_clusters = {i:[] for i in dic_clusters.keys()}\n",
    "\n",
    "for key in dic_clusters_antigens.keys():\n",
    "    for antigen in dic_clusters_antigens[key]:\n",
    "        if not antigen in dic_non_epitope[key][1]:\n",
    "            dic_epitopes_clusters[key].append(formal2informal_antigen(antigen))\n",
    "\n",
    "for cluster in dic_epitopes_clusters.keys():\n",
    "    if (len(cluster.split('*')[0]) > 1 ) and (cluster.split('*')[0][:2] in ['DQ',  'DP']):\n",
    "        locus = cluster.split('*')[0][:2]\n",
    "        for antigen in dic_epitopes_clusters[cluster]:\n",
    "            \n",
    "            list_alleles = antigen.split('-')\n",
    "            list_alleles = [f'{locus}A1*{list_alleles[0][3:5]}:{list_alleles[0][5:]}',\n",
    "                           f'{locus}B1*{list_alleles[1][:2]}:{list_alleles[1][2:]}']\n",
    "\n",
    "            for allele in list_alleles:\n",
    "                df = df_all_eplets[df_all_eplets['allele_name'] == allele]\n",
    "                list_eplets = list(df['eplet_name'].unique())\n",
    "                \n",
    "                for eplet in list_eplets:\n",
    "                    if eplet not in df_nr_dataset['eplet_name'].unique():\n",
    "                        row = df[df['eplet_name'] == eplet]\n",
    "                        list_resids = [i for i in row['eplet_residues'].iloc[0].split('(') if not ')' in i]\n",
    "                        list_resids = list_resids[0].strip().split(' ')\n",
    "                        list_resid_num = []\n",
    "                        \n",
    "                        for resid in list_resids:\n",
    "                            resid_num = ''\n",
    "                            for i in resid:\n",
    "                                if i.isdigit():\n",
    "                                    resid_num += i\n",
    "                                else:\n",
    "                                    resid_name = i\n",
    "                                    break\n",
    "                            resid_num = int(resid_num) if locus+'A' in allele.split('*')[0] else int(resid_num)+dic_locus_lens[locus+'A']\n",
    "                            patch = track_resids_patchs.loc[(track_resids_patchs['antigen'] == antigen) & \\\n",
    "                                    (track_resids_patchs['central_AA'] == resid_num), 'patch_ID']\n",
    "                            patch = patch.iloc[0] if len(patch) > 0 else float('nan')\n",
    "                            df_nr_dataset.loc[len(df_nr_dataset)] = [locus, eplet, resid_num, convert_name(antigen), patch]\n",
    "\n",
    "    else:\n",
    "        locus = cluster.split('*')[0][:2] if len(cluster.split('*')[0]) > 1 else cluster.split('*')[0]\n",
    "        for antigen in dic_epitopes_clusters[cluster]:\n",
    "            df = df_all_eplets[df_all_eplets['allele_name'] == convert_name(antigen)]\n",
    "            list_eplets = list(df['eplet_name'].unique())\n",
    "            for eplet in list_eplets:\n",
    "                if eplet not in df_nr_dataset['eplet_name'].unique():\n",
    "                    row = df[df['eplet_name'] == eplet]\n",
    "                    list_resids = [i for i in row['eplet_residues'].iloc[0].split('(') if not ')' in i]\n",
    "                    list_resids = list_resids[0].strip().split(' ')\n",
    "                    list_resid_num = []\n",
    "                    for resid in list_resids:\n",
    "                        resid_num = ''\n",
    "                        for i in resid:\n",
    "                            if i.isdigit():\n",
    "                                resid_num += i\n",
    "                            else:\n",
    "                                resid_name = i\n",
    "                                break\n",
    "                        resid_num = int(resid_num) if locus in ['A', 'B', 'C'] else int(resid_num)+dic_locus_lens['DRA']\n",
    "                        patch = track_resids_patchs.loc[(track_resids_patchs['antigen'] == antigen) & \\\n",
    "                                (track_resids_patchs['central_AA'] == int(resid_num)), 'patch_ID']\n",
    "                        patch = patch.iloc[0] if len(patch) > 0 else float('nan')\n",
    "                        df_nr_dataset.loc[len(df_nr_dataset)] = [locus, eplet, int(resid_num), convert_name(antigen), patch]\n",
    "            \n",
    "df_nr_dataset = df_nr_dataset.dropna()\n",
    "df_nr_dataset.index = range(len(df_nr_dataset))\n",
    "df_nr_dataset['class'] = [1 for i in range(len(df_nr_dataset))]\n",
    "\n",
    "\n",
    "for cluster in dic_non_epitope.keys():\n",
    "    locus = cluster.split('*')[0][:2] if len(cluster.split('*')[0]) > 1 else cluster.split('*')[0]\n",
    "    antigen = dic_non_epitope[cluster][1]\n",
    "    df_non_eplets = df_ag[(df_ag['antigen'] == antigen) & (df_ag['class'] == 0)]\n",
    "    for ix, col in df_non_eplets.iterrows():\n",
    "        resid = track_resids_patchs.loc[track_resids_patchs['patch_ID'] == col['patch_ID'], 'central_AA'].iloc[0]\n",
    "        df_nr_dataset.loc[len(df_nr_dataset)] = [locus, 'non-eplet', resid, antigen, col['patch_ID'], 0]\n",
    "\n",
    "df_nr_dataset['patch_ID'] = list(map(int, df_nr_dataset['patch_ID']))\n",
    "\n",
    "table_3_new = table_3_5[table_3_5['patch_ID'].isin(df_nr_dataset['patch_ID'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training a ML model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm can be changed. For example, you could try: RandomForestClassifier, GradientBoostingClassifier or DecisionTreeClassifier. Modify the learning parameters accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = table_3_new.sample(frac=1,random_state=42)\n",
    "\n",
    "X = dataset.iloc[:, 1:-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "model = ExtraTreesClassifier(criterion='gini', max_features=None, min_samples_leaf=1, \n",
    "                            min_samples_split=3, n_estimators=300, warm_start=False, n_jobs=-1, \n",
    "                             random_state=42)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predictions on descriptors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to load and use the previously trained models mentioned in the paper and which are located in the 'ML_models' folder.\n",
    "\n",
    "The variable 'preds' contains the predicted labels for each of the descriptors passed to the model. The label 0 corresponds to a \"Non-epitope\" prediction, while the label 1 corresponds to an \"Epitope\" prediction. The variable 'proba_preds' contains the prediction probabilities for each label of each descriptor, where the first element corresponds to the probability of label 0 and the second element to label 1.\n",
    "\n",
    "In the example below, the patch descriptors of all non-confirmed eplets in the Eplet Registry are used. These descriptors are found in the file named \"descriptors_non_confirmed_eplets.csv\" located in the 'data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with open(f'../ML_models/model_ExtraTrees_non-redundant_fulldataset.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "descriptors = pd.read_csv(f'../data/descriptors_non_confirmed_eplets.csv', sep=',', index_col=0)\n",
    "\n",
    "preds = model.predict(descriptors)\n",
    "proba_preds = model.predict_proba(descriptors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn_1_2_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
